import warnings
import os
import copy
from pathlib import Path

import lightgbm as lgb
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import xgboost as xgb
from catboost import CatBoostClassifier
from scipy import stats
from scipy.signal import savgol_filter
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, log_loss, precision_score, recall_score, cohen_kappa_score, matthews_corrcoef
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import MinMaxScaler, PowerTransformer, RobustScaler, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.feature_selection import mutual_info_classif, f_classif
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import seaborn as sns

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False
sns.set_theme(style='whitegrid')

SCRIPT_TAG = Path(__file__).stem.replace('.', '_')

def tagged_name(filename: str) -> str:
    return f"{SCRIPT_TAG}_{filename}"


FACIES_COLOR_MAP = {
    0: '#FF3B30',  # 红 - Facies 0
    1: '#007AFF',  # 蓝 - Facies 1
    2: '#34C759',  # 绿 - Facies 2
}

FACIES_NAMES = {
    0: 'Facies 0',
    1: 'Facies 1',
    2: 'Facies 2',
}

FACIES_ORDER = [0, 1, 2]

MODEL_DISPLAY_NAMES = {
    'lgb': 'LightGBM',
    'xgb': 'XGBoost',
    'cat': 'CatBoost',
    'et': 'Extra Trees',
    'rf': 'Random Forest',
    'transformer': 'Transformer',
    'cnn_lstm': 'CNN-BiLSTM',
    'ensemble_static': 'Static Weighted Ensemble',
    'ensemble_dynamic': 'Dynamic Weighted Ensemble',
}


# 设置随机种子
def set_seed(seed: int = 42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


class WellLogDataset(Dataset):
    """测井数据集"""

    def __init__(self, features, labels=None):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels) if labels is not None else None

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        if self.labels is not None:
            return self.features[idx], self.labels[idx]
        return self.features[idx]


class CNN_LSTMModel(nn.Module):
    """CNN-BiLSTM模型：适用于表格数据的卷积-长短期记忆网络"""

    def __init__(self, input_dim, hidden_dim=256, num_layers=2, num_classes=3, dropout=0.3, seq_len=16):
        super().__init__()
        
        self.input_dim = input_dim
        self.seq_len = seq_len  # 将特征分组成seq_len个序列
        self.feature_per_step = input_dim // seq_len + (1 if input_dim % seq_len != 0 else 0)
        
        # 输入投影：将不规则特征对齐
        self.input_projection = nn.Linear(input_dim, self.feature_per_step * seq_len)
        
        # 多层1D卷积特征提取
        self.conv_layers = nn.Sequential(
            # 第一层卷积
            nn.Conv1d(self.feature_per_step, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            # 第二层卷积
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            # 第三层卷积（残差连接前）
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 双向LSTM层
        self.lstm = nn.LSTM(
            hidden_dim,
            hidden_dim // 2,  # 因为是双向，最后输出是hidden_dim
            num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # 自注意力机制
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # 分类器（多层全连接）
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        batch_size = x.size(0)
        
        # 输入对齐并reshape成序列
        # [batch, input_dim] -> [batch, feature_per_step * seq_len]
        x = self.input_projection(x)
        
        # Reshape成序列形式: [batch, seq_len, feature_per_step]
        x = x.view(batch_size, self.seq_len, self.feature_per_step)
        
        # 转置为CNN输入格式: [batch, feature_per_step, seq_len]
        x = x.transpose(1, 2)
        
        # CNN特征提取: [batch, feature_per_step, seq_len] -> [batch, hidden_dim, seq_len]
        conv_out = self.conv_layers(x)
        
        # 转回序列格式: [batch, hidden_dim, seq_len] -> [batch, seq_len, hidden_dim]
        conv_out = conv_out.transpose(1, 2)
        
        # 双向LSTM处理: [batch, seq_len, hidden_dim]
        lstm_out, (h_n, c_n) = self.lstm(conv_out)
        
        # 自注意力加权
        attention_weights = self.attention(lstm_out)  # [batch, seq_len, 1]
        attention_weights = torch.softmax(attention_weights, dim=1)
        
        # 加权求和: [batch, hidden_dim]
        attended = torch.sum(lstm_out * attention_weights, dim=1)
        
        # 分类: [batch, num_classes]
        output = self.classifier(attended)
        
        return output


class TransformerBlock(nn.Module):
    """增强的Transformer块"""

    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        return x


class EnhancedTabularTransformer(nn.Module):
    """增强的表格数据Transformer"""

    def __init__(
        self,
        input_dim,
        num_classes=3,
        embed_dim=256,
        num_heads=16,
        num_layers=6,
        ff_dim=512,
        dropout=0.2,
    ):
        super().__init__()
        self.input_projection = nn.Sequential(
            nn.Linear(input_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim),
        )
        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_dim))
        self.transformer_blocks = nn.ModuleList(
            [TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)]
        )
        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.LayerNorm(embed_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, embed_dim // 4),
            nn.LayerNorm(embed_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 4, num_classes),
        )

    def forward(self, x):
        x = self.input_projection(x)
        x = x.unsqueeze(1)
        x = x + self.positional_encoding[:, : x.size(1), :]
        for transformer in self.transformer_blocks:
            x = transformer(x)
        x = x.mean(dim=1)
        output = self.classifier(x)
        return output


class UltraFastLithologyIdentifier:
    """超快速岩性识别系统 - 使用高效融合策略 + 概率校准 + 维特比转移惩罚"""

    def __init__(self):
        self.models = {}
        self.feature_columns = []
        self.scalers = {
            'robust': RobustScaler(),
            'standard': StandardScaler(),
            'minmax': MinMaxScaler(),
            'power': PowerTransformer(),
        }
        self.best_epochs = {}
        self.training_logs = {}
        self.model_performance = {}
        self.num_classes = 3

    def get_display_name(self, model_key: str) -> str:
        if model_key is None:
            return ''
        if model_key in MODEL_DISPLAY_NAMES:
            return MODEL_DISPLAY_NAMES[model_key]
        return str(model_key).replace('_', ' ').title()

    def safe_divide(self, a, b, default=0):
        """安全除法"""
        with np.errstate(divide='ignore', invalid='ignore'):
            result = np.divide(a, b)
            result = np.where(np.isfinite(result), result, default)
        return result

    def robust_clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """数据清理"""
        df = df.copy()
        df = df.replace([np.inf, -np.inf], np.nan)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in ['WELL', 'DEPTH', 'label', 'ID', 'id']:
                Q1 = df[col].quantile(0.01)
                Q3 = df[col].quantile(0.99)
                IQR = Q3 - Q1
                lower_bound = Q1 - 3 * IQR
                upper_bound = Q3 + 3 * IQR
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
                median_val = df[col].median()
                if not np.isfinite(median_val):
                    median_val = 0
                df[col] = df[col].fillna(median_val)
                df[col] = df[col].replace([np.inf, -np.inf], median_val)
        return df

    @staticmethod
    def _augment_features_window(X: np.ndarray, n_neig: int) -> np.ndarray:
        """Bestagini 等提出的窗口特征拼接"""
        n_row, n_feat = X.shape
        padded = np.vstack((np.zeros((n_neig, n_feat)), X, np.zeros((n_neig, n_feat))))
        X_aug = np.zeros((n_row, n_feat * (2 * n_neig + 1)))
        for r in np.arange(n_row) + n_neig:
            window_rows = []
            for offset in range(-n_neig, n_neig + 1):
                window_rows = np.hstack((window_rows, padded[r + offset]))
            X_aug[r - n_neig] = window_rows
        return X_aug

    @staticmethod
    def _augment_features_gradient(X: np.ndarray, depth: np.ndarray) -> np.ndarray:
        """Bestagini 等提出的梯度特征"""
        d_diff = np.diff(depth).reshape((-1, 1))
        d_diff[d_diff == 0] = 1e-3
        X_diff = np.diff(X, axis=0)
        X_grad = X_diff / d_diff
        X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))
        return X_grad

    def _bestagini_augment(self, df: pd.DataFrame, feature_cols: list, n_neig: int = 2) -> pd.DataFrame:
        """对指定特征执行 Bestagini 式的窗口+梯度增强"""
        wells = df['WELL'].values
        depths = df['DEPTH'].values
        X = df[feature_cols].values

        window_feats = self._augment_features_window(X, n_neig)
        grad_feats = self._augment_features_gradient(X, depths)

        augmented = np.concatenate((window_feats, grad_feats), axis=1)

        col_names = []
        offsets = list(range(-n_neig, n_neig + 1))
        for feature in feature_cols:
            for offset in offsets:
                suffix = f"{offset:+d}".replace('+', 'p').replace('-', 'm')
                col_names.append(f"{feature}_win_{suffix}")
            col_names.append(f"{feature}_grad")

        aug_df = pd.DataFrame(augmented, columns=col_names, index=df.index)
        return aug_df

    def create_ultra_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """采用 Bestagini 特征增强 + 衍生特征，使特征工程与参考方案一致"""
        df = df.copy()
        exclude_cols = {'WELL', 'DEPTH', 'label', 'ID', 'id'}
        value_cols = [col for col in df.columns if col not in exclude_cols]
        base_cols = [col for col in ['SP', 'GR', 'AC'] if col in value_cols]
        density_cols = [col for col in ['RHOB', 'DEN'] if col in value_cols]
        neutron_cols = [col for col in ['NPHI', 'CNL'] if col in value_cols]
        resistivity_cols = [col for col in ['LLD', 'LLS', 'MSFL', 'RT', 'RDEP'] if col in value_cols]

        if not value_cols:
            raise ValueError("输入数据缺少可用的数值特征")

        for col in value_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # 保存原始索引，按照 WELL-DEPTH 排序以保证序列连续性
        df_sorted = df.sort_values(['WELL', 'DEPTH']).reset_index()
        sorted_index = df_sorted['index']
        df_sorted[value_cols] = df_sorted[value_cols].astype(float)

        # 基础填充：按井分组做前后向填充，再使用 0 替代缺失
        for feature in value_cols:
            df_sorted[feature] = (
                df_sorted.groupby('WELL')[feature]
                .apply(lambda s: s.ffill().bfill())
                .reset_index(level=0, drop=True)
            )
        df_sorted[value_cols] = df_sorted[value_cols].fillna(0.0)

        # Bestagini 特征增强
        print("执行 Bestagini 滑动窗口增强特征...")
        aug_df = self._bestagini_augment(df_sorted, value_cols, n_neig=2)
        aug_df.index = sorted_index

        # 汇总统计特征（针对增强前的原始数据）
        print("计算局部统计特征...")
        stats_data = {}
        window_sizes = [5, 10, 25]
        for feature in value_cols:
            for window in window_sizes:
                stats_data[f'{feature}_mean_{window}'] = (
                    df_sorted.groupby('WELL')[feature]
                    .transform(lambda x: x.rolling(window=window, min_periods=1, center=True).mean())
                    .values
                )
                stats_data[f'{feature}_std_{window}'] = (
                    df_sorted.groupby('WELL')[feature]
                    .transform(lambda x: x.rolling(window=window, min_periods=1, center=True).std())
                    .fillna(0)
                    .values
                )
                stats_data[f'{feature}_min_{window}'] = (
                    df_sorted.groupby('WELL')[feature]
                    .transform(lambda x: x.rolling(window=window, min_periods=1, center=True).min())
                    .values
                )
                stats_data[f'{feature}_max_{window}'] = (
                    df_sorted.groupby('WELL')[feature]
                    .transform(lambda x: x.rolling(window=window, min_periods=1, center=True).max())
                    .values
                )

        stats_df = pd.DataFrame(stats_data, index=sorted_index)

        # 差分及比值特征
        print("构建梯度与比值特征...")
        diff_data = {}
        for feature in value_cols:
            diff_series = df_sorted.groupby('WELL')[feature].diff().fillna(0)
            pct_series = (
                df_sorted.groupby('WELL')[feature]
                .pct_change()
                .replace([np.inf, -np.inf], 0)
                .fillna(0)
            )
            diff_data[f'{feature}_diff1'] = diff_series.values
            diff_data[f'{feature}_pct1'] = pct_series.values

        diff_df = pd.DataFrame(diff_data, index=sorted_index)

        # 重新加入原始工程中大量滑窗 / 差分 / 交叉特征
        original_feat_data = {}
        if base_cols:
            print("生成大规模滑窗统计特征...")
            for feature in base_cols:
                grouped = df_sorted.groupby('WELL')[feature]
                for window in range(1, 64):
                    mean_vals = grouped.transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).mean()
                    )
                    std_vals = grouped.transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).std()
                    ).fillna(0)
                    median_vals = grouped.transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).median()
                    )
                    original_feat_data[f'{feature}_roll_mean_{window}'] = mean_vals.values
                    original_feat_data[f'{feature}_roll_std_{window}'] = std_vals.values
                    original_feat_data[f'{feature}_roll_median_{window}'] = median_vals.values
                    original_feat_data[f'{feature}_vs_roll_mean_{window}'] = self.safe_divide(
                        df_sorted[feature].values,
                        mean_vals.values + 1e-8,
                    )
                    original_feat_data[f'{feature}_zscore_{window}'] = self.safe_divide(
                        df_sorted[feature].values - mean_vals.values,
                        std_vals.values + 1e-8,
                    )

            print("生成高阶差分特征...")
            for feature in base_cols:
                grouped = df_sorted.groupby('WELL')[feature]
                for lag in range(1, 16):
                    diff_vals = grouped.diff(lag).fillna(0)
                    pct_vals = grouped.pct_change(lag).replace([np.inf, -np.inf], 0).fillna(0)
                    original_feat_data[f'{feature}_diff_{lag}'] = diff_vals.values
                    original_feat_data[f'{feature}_pct_{lag}'] = pct_vals.values

            print("生成地质物理与交叉特征...")
            geo_data = {}
            if 'GR' in base_cols:
                gr_group = df_sorted.groupby('WELL')['GR']
                gr_min = gr_group.transform('min')
                gr_max = gr_group.transform('max')
                vsh_linear = pd.Series(
                    self.safe_divide(df_sorted['GR'] - gr_min, gr_max - gr_min),
                    index=df_sorted.index,
                )
                vsh_linear = vsh_linear.clip(0, 1)
                geo_data['Vsh_linear'] = vsh_linear
                geo_data['Vsh_larionov_old'] = 0.33 * (2 ** (2 * vsh_linear) - 1)
                geo_data['Vsh_larionov_tertiary'] = 0.083 * (2 ** (3.7 * vsh_linear) - 1)

            if 'AC' in base_cols:
                phi_wyllie = pd.Series(
                    self.safe_divide(df_sorted['AC'] - 180, 300 - 180),
                    index=df_sorted.index,
                ).clip(0, 1)
                phi_raymer = (0.67 * self.safe_divide(df_sorted['AC'] - 180, df_sorted['AC']))
                phi_raymer = pd.Series(phi_raymer, index=df_sorted.index).clip(0, 1)
                geo_data['PHI_wyllie'] = phi_wyllie
                geo_data['PHI_raymer'] = phi_raymer

            cross_data = {}
            feature_pairs = [('GR', 'AC'), ('GR', 'SP'), ('AC', 'SP')]
            for f1, f2 in feature_pairs:
                if f1 in df_sorted.columns and f2 in df_sorted.columns:
                    cross_data[f'{f1}_{f2}_product'] = (df_sorted[f1] * df_sorted[f2])
                    cross_data[f'{f1}_{f2}_ratio'] = pd.Series(
                        self.safe_divide(df_sorted[f1], df_sorted[f2] + 1e-8),
                        index=df_sorted.index,
                    )
                    cross_data[f'{f1}_{f2}_mean'] = (df_sorted[f1] + df_sorted[f2]) / 2

            poly_data = {}
            for feature in base_cols:
                poly_data[f'{feature}_squared'] = df_sorted[feature] ** 2
                poly_data[f'{feature}_sqrt'] = np.sqrt(np.abs(df_sorted[feature]))
                shifted = df_sorted[feature] - df_sorted[feature].min() + 1e-8
                poly_data[f'{feature}_log'] = np.log1p(np.maximum(0, shifted))

            rank_data = {}
            for feature in base_cols:
                rank_series = df_sorted.groupby('WELL')[feature].rank(pct=True)
                rank_data[f'{feature}_rank'] = rank_series.values

            detailed_feat_df = pd.DataFrame(original_feat_data, index=sorted_index)
            geo_feat_df = pd.DataFrame(geo_data, index=sorted_index) if geo_data else pd.DataFrame(index=sorted_index)
            cross_feat_df = pd.DataFrame(cross_data, index=sorted_index) if cross_data else pd.DataFrame(index=sorted_index)
            poly_feat_df = pd.DataFrame(poly_data, index=sorted_index)
            rank_feat_df = pd.DataFrame(rank_data, index=sorted_index)
        else:
            detailed_feat_df = pd.DataFrame(index=sorted_index)
            geo_feat_df = pd.DataFrame(index=sorted_index)
            cross_feat_df = pd.DataFrame(index=sorted_index)
            poly_feat_df = pd.DataFrame(index=sorted_index)
            rank_feat_df = pd.DataFrame(index=sorted_index)

        # 合并所有特征并恢复原始顺序
        combined = pd.concat([
            df,
            aug_df.loc[df.index],
            stats_df.loc[df.index],
            diff_df.loc[df.index],
            detailed_feat_df.loc[df.index],
            geo_feat_df.loc[df.index],
            cross_feat_df.loc[df.index],
            poly_feat_df.loc[df.index],
            rank_feat_df.loc[df.index],
        ], axis=1)

        # 生成深度衍生特征
        print("生成深度衍生特征...")
        combined['DEPTH_normalized'] = combined.groupby('WELL')['DEPTH'].transform(
            lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
        )
        combined['DEPTH_gradient'] = combined.groupby('WELL')['DEPTH'].diff().fillna(0)
        combined['DEPTH_sin'] = np.sin(np.pi * combined['DEPTH_normalized'])
        combined['DEPTH_cos'] = np.cos(np.pi * combined['DEPTH_normalized'])

        # 生成区分性特征：专门用于区分粉砂岩(0)、砂岩(1)和泥岩(2)
        # 基于三种岩性的物理差异：粒度、孔隙度、渗透率、矿物成分等
        print("生成三分类岩性区分性特征...")
        disc_features = {}
        
        if base_cols:
            # ========== 1. 粒度分布特征 ==========
            # 粉砂岩(0): 0.004-0.0625mm, 砂岩(1): 0.0625-2mm, 泥岩(2): <0.004mm
            if 'GR' in base_cols and 'AC' in base_cols:
                grain_ratio = pd.Series(
                    self.safe_divide(df_sorted['GR'], df_sorted['AC'] + 1e-8),
                    index=df_sorted.index,
                )
                disc_features['grain_size_indicator_GR_AC'] = grain_ratio
                disc_features['grain_size_product_GR_AC'] = df_sorted['GR'] * df_sorted['AC']
                disc_features['grain_size_difference_GR_AC'] = df_sorted['GR'] - df_sorted['AC']
                
                # 粒度分级特征：基于GR和AC的组合判断粒度大小
                gr_norm = df_sorted.groupby('WELL')['GR'].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                )
                ac_norm = df_sorted.groupby('WELL')['AC'].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                )
                # 细粒度指示（泥岩+粉砂岩）：GR高、AC低
                disc_features['fine_grain_indicator'] = gr_norm * (1 - ac_norm)
                # 粗粒度指示（砂岩）：GR低、AC高
                disc_features['coarse_grain_indicator'] = (1 - gr_norm) * ac_norm
                # 中等粒度指示（粉砂岩）：GR中等、AC中等
                disc_features['medium_grain_indicator'] = (1 - np.abs(gr_norm - 0.5)) * (1 - np.abs(ac_norm - 0.5))
            
            # ========== 2. 矿物成分特征 ==========
            # 泥岩：主要由粘土矿物组成，GR值高
            # 粉砂岩：主要由石英和长石组成
            # 砂岩：主要由石英、长石和岩屑组成，石英含量高
            if 'GR' in base_cols:
                gr_group = df_sorted.groupby('WELL')['GR']
                gr_min = gr_group.transform('min')
                gr_max = gr_group.transform('max')
                vsh_linear = pd.Series(
                    self.safe_divide(df_sorted['GR'] - gr_min, gr_max - gr_min),
                    index=df_sorted.index,
                ).clip(0, 1)
                
                # 粘土含量指示（泥岩特征）
                disc_features['clay_content_indicator'] = vsh_linear
                disc_features['clay_content_squared'] = vsh_linear ** 2
                
                # 石英含量指示（砂岩特征）：GR低表示石英含量高
                disc_features['quartz_content_indicator'] = 1 - vsh_linear
                
            if 'GR' in base_cols and 'SP' in base_cols:
                # SP与GR的组合可以更好地区分矿物成分
                sp_group = df_sorted.groupby('WELL')['SP']
                sp_min = sp_group.transform('min')
                sp_max = sp_group.transform('max')
                sp_norm = pd.Series(
                    self.safe_divide(df_sorted['SP'] - sp_min, sp_max - sp_min),
                    index=df_sorted.index,
                ).clip(0, 1)
                
                # 矿物成分综合指标
                disc_features['mineral_composition_GR_SP'] = vsh_linear - sp_norm
                disc_features['mineral_composition_product'] = vsh_linear * (1 - sp_norm)
            
            # ========== 3. 孔隙度区分特征 ==========
            # 粉砂岩孔隙度通常比砂岩低，泥岩孔隙度最低
            ac_normalized = None
            if 'AC' in base_cols:
                ac_normalized = df_sorted.groupby('WELL')['AC'].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                )
                disc_features['porosity_indicator_AC_norm'] = ac_normalized
                
                # 孔隙度分级：高孔隙度（砂岩）、中等孔隙度（粉砂岩）、低孔隙度（泥岩）
                disc_features['high_porosity_indicator'] = (ac_normalized > 0.6).astype(float)
                disc_features['low_porosity_indicator'] = (ac_normalized < 0.4).astype(float)
                disc_features['medium_porosity_indicator'] = ((ac_normalized >= 0.4) & (ac_normalized <= 0.6)).astype(float)
            
            # ========== 4. 物理性质组合特征 ==========
            if 'GR' in base_cols and 'SP' in base_cols and 'AC' in base_cols:
                gr_norm = df_sorted.groupby('WELL')['GR'].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                )
                sp_norm = df_sorted.groupby('WELL')['SP'].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                )
                
                # 三分类综合指标
                disc_features['lithology_discriminator_GR_SP'] = gr_norm - sp_norm
                disc_features['lithology_discriminator_combined'] = (
                    gr_norm * 0.5 + (1 - sp_norm) * 0.3 + ac_normalized * 0.2
                )
                disc_features['tri_lithology_indicator'] = (
                    df_sorted['GR'] + df_sorted['AC'] - df_sorted['SP']
                )
                
                # 泥岩指示器：GR高、SP低、AC低
                disc_features['mudstone_indicator'] = gr_norm * (1 - sp_norm) * (1 - ac_normalized)
                # 砂岩指示器：GR低、SP高、AC高
                disc_features['sandstone_indicator'] = (1 - gr_norm) * sp_norm * ac_normalized
                # 粉砂岩指示器：GR中等、SP中等、AC中等
                disc_features['siltstone_indicator'] = (
                    (1 - np.abs(gr_norm - 0.5)) * 
                    (1 - np.abs(sp_norm - 0.5)) * 
                    (1 - np.abs(ac_normalized - 0.5))
                )
            
            # ========== 5. 局部纹理特征 ==========
            # 粉砂岩通常更均匀，砂岩可能更不均匀，泥岩可能具有层理
            for feature in base_cols:
                if feature in df_sorted.columns:
                    # 局部变异性
                    for window in [3, 5, 7, 10]:
                        local_var = df_sorted.groupby('WELL')[feature].transform(
                            lambda x: x.rolling(window=window, min_periods=1, center=True).std()
                        ).fillna(0)
                        disc_features[f'{feature}_local_variability_{window}'] = local_var.values
                        
                        # 变异系数（标准差/均值）
                        local_mean = df_sorted.groupby('WELL')[feature].transform(
                            lambda x: x.rolling(window=window, min_periods=1, center=True).mean()
                        )
                        cv = self.safe_divide(local_var.values, local_mean.values + 1e-8)
                        disc_features[f'{feature}_coefficient_variation_{window}'] = cv
                    
                    # 局部梯度
                    local_grad = df_sorted.groupby('WELL')[feature].diff().abs()
                    disc_features[f'{feature}_local_gradient'] = local_grad.values
                    
                    # 局部梯度变化率（二阶梯度）
                    local_grad2 = df_sorted.groupby('WELL')[feature].diff().diff().abs()
                    disc_features[f'{feature}_local_gradient2'] = local_grad2.fillna(0).values
            
            # ========== 6. 多尺度统计对比特征 ==========
            for window in [3, 5, 7, 10, 15]:
                for feature in base_cols:
                    if feature in df_sorted.columns:
                        # 局部均值与全局均值的对比
                        local_mean = df_sorted.groupby('WELL')[feature].transform(
                            lambda x: x.rolling(window=window, min_periods=1, center=True).mean()
                        )
                        global_mean = df_sorted.groupby('WELL')[feature].transform('mean')
                        disc_features[f'{feature}_local_vs_global_mean_{window}'] = self.safe_divide(
                            local_mean.values - global_mean.values,
                            global_mean.values + 1e-8
                        )
                        
                        # 局部中位数与全局中位数的对比
                        local_median = df_sorted.groupby('WELL')[feature].transform(
                            lambda x: x.rolling(window=window, min_periods=1, center=True).median()
                        )
                        global_median = df_sorted.groupby('WELL')[feature].transform('median')
                        disc_features[f'{feature}_local_vs_global_median_{window}'] = self.safe_divide(
                            local_median.values - global_median.values,
                            global_median.values + 1e-8
                        )
            
            # ========== 7. 深度相关特征 ==========
            # 不同深度段的岩性分布可能不同
            if 'DEPTH' in df_sorted.columns:
                depth_normalized = df_sorted.groupby('WELL')['DEPTH'].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                )
                
                # 深度段指示器
                disc_features['depth_shallow'] = (depth_normalized < 0.33).astype(float)
                disc_features['depth_middle'] = ((depth_normalized >= 0.33) & (depth_normalized < 0.67)).astype(float)
                disc_features['depth_deep'] = (depth_normalized >= 0.67).astype(float)
                
                # 深度与GR的交互特征（不同深度段的GR响应可能不同）
                if 'GR' in base_cols:
                    disc_features['depth_GR_interaction'] = depth_normalized * df_sorted.groupby('WELL')['GR'].transform(
                        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)
                    )
        
        disc_feat_df = pd.DataFrame(disc_features, index=sorted_index) if disc_features else pd.DataFrame(index=sorted_index)

        extra_feat_df = pd.DataFrame(index=sorted_index)

        # ========== 电阻率特征增强 ==========
        # 电阻率可以反映孔隙度、渗透率和流体性质
        # 泥岩：电阻率通常较低（高粘土含量）
        # 砂岩：电阻率可能较高（低粘土含量，高孔隙度）
        # 粉砂岩：电阻率介于两者之间
        if resistivity_cols:
            if 'LLD' in df_sorted.columns and 'LLS' in df_sorted.columns:
                extra_feat_df['res_ratio_LLD_LLS'] = pd.Series(
                    self.safe_divide(df_sorted['LLD'], df_sorted['LLS'] + 1e-8),
                    index=df_sorted.index,
                )
                extra_feat_df['res_logdiff_LLD_LLS'] = (
                    np.log10(np.abs(df_sorted['LLD']) + 1e-8) - np.log10(np.abs(df_sorted['LLS']) + 1e-8)
                )
                # 电阻率侵入特征：反映渗透率和孔隙度
                extra_feat_df['res_invasion_indicator'] = self.safe_divide(
                    df_sorted['LLD'] - df_sorted['LLS'],
                    df_sorted['LLD'] + 1e-8
                )
                
                # 电阻率梯度特征
                res_grad_LLD = df_sorted.groupby('WELL')['LLD'].diff().fillna(0)
                res_grad_LLS = df_sorted.groupby('WELL')['LLS'].diff().fillna(0)
                extra_feat_df['res_grad_LLD'] = res_grad_LLD.values
                extra_feat_df['res_grad_LLS'] = res_grad_LLS.values
                
                # 电阻率局部统计特征
                for window in [5, 10]:
                    res_local_std_LLD = df_sorted.groupby('WELL')['LLD'].transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).std()
                    ).fillna(0)
                    res_local_std_LLS = df_sorted.groupby('WELL')['LLS'].transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).std()
                    ).fillna(0)
                    extra_feat_df[f'res_local_std_LLD_{window}'] = res_local_std_LLD.values
                    extra_feat_df[f'res_local_std_LLS_{window}'] = res_local_std_LLS.values
            
            if 'MSFL' in df_sorted.columns and 'LLD' in df_sorted.columns:
                extra_feat_df['res_ratio_MSFL_LLD'] = self.safe_divide(df_sorted['MSFL'], df_sorted['LLD'] + 1e-8)
                # 浅电阻率与深电阻率的差异（反映侵入带特征）
                extra_feat_df['res_shallow_deep_diff'] = df_sorted['MSFL'] - df_sorted['LLD']
                extra_feat_df['res_grad_MSFL'] = df_sorted.groupby('WELL')['MSFL'].diff().fillna(0).values
            
            if 'RT' in df_sorted.columns:
                extra_feat_df['res_log_RT'] = np.log10(np.abs(df_sorted['RT']) + 1e-8)
                extra_feat_df['res_grad_RT'] = df_sorted.groupby('WELL')['RT'].diff().fillna(0).values
                
                # RT的局部统计特征
                for window in [5, 10]:
                    rt_local_mean = df_sorted.groupby('WELL')['RT'].transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).mean()
                    )
                    rt_local_std = df_sorted.groupby('WELL')['RT'].transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).std()
                    ).fillna(0)
                    extra_feat_df[f'res_RT_local_mean_{window}'] = rt_local_mean.values
                    extra_feat_df[f'res_RT_local_std_{window}'] = rt_local_std.values
            
            if 'RDEP' in df_sorted.columns and 'RT' in df_sorted.columns:
                extra_feat_df['res_ratio_RDEP_RT'] = self.safe_divide(df_sorted['RDEP'], df_sorted['RT'] + 1e-8)

        # ========== 密度-中子-声波组合特征 ==========
        # 这些特征可以更好地估计孔隙度和岩性
        phi_density = None
        if density_cols:
            rhob_series = df_sorted[density_cols[0]]
            phi_density = pd.Series(
                self.safe_divide(2.65 - rhob_series, 2.65 - 1.0),
                index=df_sorted.index,
            )
            extra_feat_df['phi_density_est'] = phi_density.clip(-0.1, 0.6)
            
            # 密度梯度特征
            extra_feat_df['rhob_grad'] = df_sorted.groupby('WELL')[rhob_series.name].diff().fillna(0).values
            
            # 密度与声波时差的组合（可以更好地区分岩性）
            if 'AC' in base_cols:
                extra_feat_df['rhob_ac_product'] = rhob_series * df_sorted['AC']
                extra_feat_df['rhob_ac_ratio'] = self.safe_divide(rhob_series, df_sorted['AC'] + 1e-8)
                # 密度-声波时差交会图特征
                extra_feat_df['rhob_ac_crossplot'] = (
                    (rhob_series - rhob_series.mean()) * (df_sorted['AC'] - df_sorted['AC'].mean())
                )
        
        if neutron_cols:
            nphi_series = df_sorted[neutron_cols[0]]
            extra_feat_df['nphi_grad'] = df_sorted.groupby('WELL')[nphi_series.name].diff().fillna(0).values
            
            if phi_density is not None:
                extra_feat_df['delta_phi_NPHI_density'] = nphi_series - phi_density
                # 密度-中子孔隙度差异（可以区分含气层和泥质层）
                extra_feat_df['phi_density_nphi_diff'] = np.abs(phi_density - nphi_series)
            
            # 中子与声波时差的组合
            if 'AC' in base_cols:
                extra_feat_df['nphi_ac_product'] = nphi_series * df_sorted['AC']
                extra_feat_df['nphi_ac_ratio'] = self.safe_divide(nphi_series, df_sorted['AC'] + 1e-8)
            
            # 密度-中子组合
            if phi_density is not None:
                extra_feat_df['rhob_nphi_product'] = rhob_series * nphi_series
                extra_feat_df['rhob_nphi_ratio'] = self.safe_divide(rhob_series, nphi_series + 1e-8)

        # ========== 增强的统计特征 ==========
        # 分位数特征（更多分位数和窗口）
        quantile_windows = [5, 7, 9, 11, 15]
        quantile_levels = [0.1, 0.25, 0.5, 0.75, 0.9]
        for feature in base_cols:
            for window in quantile_windows:
                for q_level in quantile_levels:
                    q_val = df_sorted.groupby('WELL')[feature].transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).quantile(q_level)
                    )
                    extra_feat_df[f'{feature}_roll_q{int(q_level*100)}_{window}'] = q_val.values
                
                # 分位数间距（IQR等）
                q25 = df_sorted.groupby('WELL')[feature].transform(
                    lambda x: x.rolling(window=window, min_periods=1, center=True).quantile(0.25)
                )
                q75 = df_sorted.groupby('WELL')[feature].transform(
                    lambda x: x.rolling(window=window, min_periods=1, center=True).quantile(0.75)
                )
                extra_feat_df[f'{feature}_roll_iqr_{window}'] = (q75 - q25).values
                extra_feat_df[f'{feature}_roll_qrange_{window}'] = (
                    df_sorted.groupby('WELL')[feature].transform(
                        lambda x: x.rolling(window=window, min_periods=1, center=True).quantile(0.9) -
                                  x.rolling(window=window, min_periods=1, center=True).quantile(0.1)
                    )
                ).values
            
            # 偏度和峰度（更多窗口）
            for window in [5, 7, 9, 11, 15]:
                skew_series = df_sorted.groupby('WELL')[feature].transform(
                    lambda x: x.rolling(window=window, min_periods=1, center=True).skew()
                )
                kurt_series = df_sorted.groupby('WELL')[feature].transform(
                    lambda x: x.rolling(window=window, min_periods=1, center=True).kurt()
                )
                extra_feat_df[f'{feature}_roll_skew_{window}'] = skew_series.fillna(0).values
                extra_feat_df[f'{feature}_roll_kurt_{window}'] = kurt_series.fillna(0).values
            
            # 更多统计量：最大值、最小值、范围
            for window in [5, 10, 15]:
                roll_max = df_sorted.groupby('WELL')[feature].transform(
                    lambda x: x.rolling(window=window, min_periods=1, center=True).max()
                )
                roll_min = df_sorted.groupby('WELL')[feature].transform(
                    lambda x: x.rolling(window=window, min_periods=1, center=True).min()
                )
                extra_feat_df[f'{feature}_roll_max_{window}'] = roll_max.values
                extra_feat_df[f'{feature}_roll_min_{window}'] = roll_min.values
                extra_feat_df[f'{feature}_roll_range_{window}'] = (roll_max - roll_min).values

        combined = pd.concat([combined, disc_feat_df.loc[combined.index], extra_feat_df.loc[combined.index]], axis=1)

        # 最终清理
        print("执行最终数据清理...")
        combined = self.robust_clean_data(combined)

        valid_cols = [col for col in combined.columns if col not in ['WELL', 'DEPTH', 'label', 'ID', 'id']]
        print(f"特征工程完成，总特征数: {len(valid_cols)}")
        print("前10个特征:", valid_cols[:10])
        return combined

    def _attach_two_stage_features(
        self,
        full_train_df: pd.DataFrame,
        test_df: pd.DataFrame,
        train_indices: pd.Index,
    ) -> None:
        """拟合训练子集的两阶段模型，并为训练/测试生成概率特征。"""

        feature_candidates = [
            'GR', 'SP', 'AC', 'RHOB', 'LLD', 'LLS', 'MSFL', 'RT', 'RDEP',
            'DEPTH_gradient', 'DEPTH_sin', 'DEPTH_cos',
            'grain_size_indicator_GR_AC', 'grain_size_product_GR_AC', 'grain_size_difference_GR_AC',
            'gr_minus_sp', 'ac_minus_sp', 'tri_lithology_indicator',
            'res_ratio_LLD_LLS', 'res_logdiff_LLD_LLS', 'res_ratio_MSFL_LLD', 'res_log_RT', 'res_ratio_RDEP_RT',
            'phi_density_est', 'delta_phi_NPHI_density', 'nphi_grad',
            'GR_roll_q25_7', 'GR_roll_q75_7', 'GR_roll_skew_9',
        ]

        available_cols = [
            col for col in feature_candidates
            if col in full_train_df.columns and (test_df is None or col in test_df.columns)
        ]

        target_cols = [
            'two_stage_prob_mud',
            'two_stage_prob_sandstone',
            'two_stage_prob_siltstone',
            'two_stage_lda_component',
        ]

        if not available_cols:
            print("警告: 两阶段特征生成失败，缺少必要的原始曲线。")
            for col in target_cols:
                full_train_df[col] = 0.0
                if test_df is not None:
                    test_df[col] = 0.0
            return

        def prepare_matrix(df: pd.DataFrame) -> np.ndarray:
            return np.nan_to_num(df[available_cols].values, nan=0.0, posinf=0.0, neginf=0.0)

        train_subset = full_train_df.loc[train_indices]
        X_subset = prepare_matrix(train_subset)
        y_subset = train_subset['label'].values

        X_full = prepare_matrix(full_train_df)
        X_test = prepare_matrix(test_df) if test_df is not None else None

        # 阶段一：泥岩检测
        mud_pipeline = Pipeline([
            ('scale', RobustScaler()),
            ('clf', LogisticRegression(class_weight='balanced', max_iter=200, random_state=42)),
        ])
        mud_pipeline.fit(X_subset, (y_subset == 2).astype(int))
        full_train_df['two_stage_prob_mud'] = mud_pipeline.predict_proba(X_full)[:, 1]
        if X_test is not None:
            test_df['two_stage_prob_mud'] = mud_pipeline.predict_proba(X_test)[:, 1]

        # 阶段二：粉砂岩 vs 砂岩
        sand_prob_train = np.zeros(len(full_train_df))
        sand_prob_test = np.zeros(len(test_df)) if X_test is not None else None
        lda_component_train = np.zeros(len(full_train_df))
        lda_component_test = np.zeros(len(test_df)) if X_test is not None else None

        non_mud_mask = y_subset != 2
        if non_mud_mask.sum() >= 10 and len(np.unique(y_subset[non_mud_mask])) == 2:
            lda = LinearDiscriminantAnalysis(n_components=1)
            lda.fit(X_subset[non_mud_mask], y_subset[non_mud_mask])
            lda_full = lda.transform(X_full)[:, 0]
            sand_pipeline = Pipeline([
                ('scale', RobustScaler()),
                ('clf', LogisticRegression(class_weight='balanced', max_iter=200, random_state=24)),
            ])
            sand_pipeline.fit(
                np.column_stack([X_subset[non_mud_mask], lda.transform(X_subset[non_mud_mask])[:, 0]]),
                y_subset[non_mud_mask],
            )
            sand_prob_train = sand_pipeline.predict_proba(np.column_stack([X_full, lda_full]))[:, 1]
            lda_component_train = lda_full

            if X_test is not None:
                lda_test = lda.transform(X_test)[:, 0]
                sand_prob_test = sand_pipeline.predict_proba(np.column_stack([X_test, lda_test]))[:, 1]
                lda_component_test = lda_test
        else:
            print("警告: 非泥岩样本不足，无法训练粉砂岩/砂岩判别模型。")

        full_train_df['two_stage_prob_sandstone'] = sand_prob_train
        full_train_df['two_stage_prob_siltstone'] = np.clip(1.0 - sand_prob_train, 0.0, 1.0)
        full_train_df['two_stage_lda_component'] = lda_component_train

        if X_test is not None:
            test_df['two_stage_prob_sandstone'] = sand_prob_test
            test_df['two_stage_prob_siltstone'] = np.clip(1.0 - sand_prob_test, 0.0, 1.0)
            test_df['two_stage_lda_component'] = lda_component_test

    def train_tree_models(self, X_train, y_train, X_val, y_val):
        """训练树模型（XGBoost, LightGBM, CatBoost, ExtraTrees, RandomForest）"""
        print("\n训练树模型...")

        # 数据清理 - 分别清理训练集和验证集，避免数据泄漏
        X_train_clean = self.robust_clean_data(X_train.copy())
        X_val_clean = self.robust_clean_data(X_val.copy())
        models = {}

        # LightGBM
        print("训练LightGBM...")
        lgb_model = lgb.LGBMClassifier(
            objective='multiclass',
            num_class=self.num_classes,
            learning_rate=0.02,
            num_leaves=31,
            max_depth=8,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            n_estimators=500,
            verbose=-1,
        )
        lgb_model.fit(
            X_train_clean,
            y_train,
            eval_set=[(X_val_clean, y_val)],
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )
        val_pred = lgb_model.predict(X_val_clean)
        val_f1 = f1_score(y_val, val_pred, average='macro')
        val_acc = accuracy_score(y_val, val_pred)
        train_pred = lgb_model.predict(X_train_clean)
        train_acc = accuracy_score(y_train, train_pred)
        train_f1 = f1_score(y_train, train_pred, average='macro')
        train_probs = lgb_model.predict_proba(X_train_clean)
        val_probs = lgb_model.predict_proba(X_val_clean)
        train_loss = log_loss(y_train, train_probs, labels=np.arange(self.num_classes))
        val_loss = log_loss(y_val, val_probs, labels=np.arange(self.num_classes))
        models['lgb'] = {'model': lgb_model, 'score': val_f1}
        self.model_performance['lgb'] = {
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
        }
        self.training_logs['lgb'] = {
            'epochs': [0, 1],
            'train_acc': [train_acc, train_acc],
            'val_acc': [val_acc, val_acc],
            'train_loss': [train_loss, train_loss],
            'val_loss': [val_loss, val_loss],
        }
        print(f"LightGBM验证F1: {val_f1:.4f}, 准确率: {val_acc:.4f}")

        # XGBoost
        print("训练XGBoost...")
        xgb_model = xgb.XGBClassifier(
            objective='multi:softprob',
            num_class=self.num_classes,
            learning_rate=0.02,
            max_depth=8,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            n_estimators=500,
            tree_method='hist',
            verbosity=0,
            early_stopping_rounds=100,
        )
        xgb_model.fit(
            X_train_clean,
            y_train,
            eval_set=[(X_val_clean, y_val)],
            verbose=False,
        )
        val_pred = xgb_model.predict(X_val_clean)
        val_f1 = f1_score(y_val, val_pred, average='macro')
        val_acc = accuracy_score(y_val, val_pred)
        train_pred = xgb_model.predict(X_train_clean)
        train_acc = accuracy_score(y_train, train_pred)
        train_f1 = f1_score(y_train, train_pred, average='macro')
        train_probs = xgb_model.predict_proba(X_train_clean)
        val_probs = xgb_model.predict_proba(X_val_clean)
        train_loss = log_loss(y_train, train_probs, labels=np.arange(self.num_classes))
        val_loss = log_loss(y_val, val_probs, labels=np.arange(self.num_classes))
        models['xgb'] = {'model': xgb_model, 'score': val_f1}
        self.model_performance['xgb'] = {
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
        }
        self.training_logs['xgb'] = {
            'epochs': [0, 1],
            'train_acc': [train_acc, train_acc],
            'val_acc': [val_acc, val_acc],
            'train_loss': [train_loss, train_loss],
            'val_loss': [val_loss, val_loss],
        }
        print(f"XGBoost验证F1: {val_f1:.4f}, 准确率: {val_acc:.4f}")

        # CatBoost
        print("训练CatBoost...")
        cat_model = CatBoostClassifier(
            loss_function='MultiClass',
            learning_rate=0.03,
            depth=6,
            l2_leaf_reg=5,
            random_seed=42,
            iterations=500,
            early_stopping_rounds=100,
            verbose=False,
        )
        cat_model.fit(
            X_train_clean,
            y_train,
            eval_set=[(X_val_clean, y_val)],
            verbose=False,
        )
        val_pred = cat_model.predict(X_val_clean)
        val_f1 = f1_score(y_val, val_pred, average='macro')
        val_acc = accuracy_score(y_val, val_pred)
        train_pred = cat_model.predict(X_train_clean)
        train_acc = accuracy_score(y_train, train_pred)
        train_f1 = f1_score(y_train, train_pred, average='macro')
        train_probs = cat_model.predict_proba(X_train_clean)
        val_probs = cat_model.predict_proba(X_val_clean)
        train_loss = log_loss(y_train, train_probs, labels=np.arange(self.num_classes))
        val_loss = log_loss(y_val, val_probs, labels=np.arange(self.num_classes))
        models['cat'] = {'model': cat_model, 'score': val_f1}
        self.model_performance['cat'] = {
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
        }
        self.training_logs['cat'] = {
            'epochs': [0, 1],
            'train_acc': [train_acc, train_acc],
            'val_acc': [val_acc, val_acc],
            'train_loss': [train_loss, train_loss],
            'val_loss': [val_loss, val_loss],
        }
        print(f"CatBoost验证F1: {val_f1:.4f}, 准确率: {val_acc:.4f}")

        # Extra Trees
        print("训练Extra Trees...")
        et_model = ExtraTreesClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
        )
        et_model.fit(X_train_clean, y_train)  # 只在训练集上训练
        val_pred = et_model.predict(X_val_clean)  # 在验证集上评估
        val_f1 = f1_score(y_val, val_pred, average='macro')
        val_acc = accuracy_score(y_val, val_pred)
        train_pred = et_model.predict(X_train_clean)
        train_acc = accuracy_score(y_train, train_pred)
        train_f1 = f1_score(y_train, train_pred, average='macro')
        train_probs = et_model.predict_proba(X_train_clean)
        val_probs = et_model.predict_proba(X_val_clean)
        train_loss = log_loss(y_train, train_probs, labels=np.arange(self.num_classes))
        val_loss = log_loss(y_val, val_probs, labels=np.arange(self.num_classes))
        models['et'] = {'model': et_model, 'score': val_f1}
        self.model_performance['et'] = {
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
        }
        self.training_logs['et'] = {
            'epochs': [0, 1],
            'train_acc': [train_acc, train_acc],
            'val_acc': [val_acc, val_acc],
            'train_loss': [train_loss, train_loss],
            'val_loss': [val_loss, val_loss],
        }
        print(f"Extra Trees验证F1: {val_f1:.4f}, 准确率: {val_acc:.4f}")

        # Random Forest
        print("训练Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=30,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
        )
        rf_model.fit(X_train_clean, y_train)  # 只在训练集上训练
        val_pred = rf_model.predict(X_val_clean)  # 在验证集上评估
        val_f1 = f1_score(y_val, val_pred, average='macro')
        val_acc = accuracy_score(y_val, val_pred)
        train_pred = rf_model.predict(X_train_clean)
        train_acc = accuracy_score(y_train, train_pred)
        train_f1 = f1_score(y_train, train_pred, average='macro')
        train_probs = rf_model.predict_proba(X_train_clean)
        val_probs = rf_model.predict_proba(X_val_clean)
        train_loss = log_loss(y_train, train_probs, labels=np.arange(self.num_classes))
        val_loss = log_loss(y_val, val_probs, labels=np.arange(self.num_classes))
        models['rf'] = {'model': rf_model, 'score': val_f1}
        self.model_performance['rf'] = {
            'train_acc': train_acc,
            'train_f1': train_f1,
            'val_acc': val_acc,
            'val_f1': val_f1,
        }
        self.training_logs['rf'] = {
            'epochs': [0, 1],
            'train_acc': [train_acc, train_acc],
            'val_acc': [val_acc, val_acc],
            'train_loss': [train_loss, train_loss],
            'val_loss': [val_loss, val_loss],
        }
        print(f"Random Forest验证F1: {val_f1:.4f}, 准确率: {val_acc:.4f}")

        return models

    def train_neural_models(self, X_train, y_train, X_val, y_val):
        """训练神经网络模型"""
        print("\n训练神经网络模型...")
        models = {}
        input_dim = X_train.shape[1]

        # 数据标准化
        scalers_fitted = {}
        for scaler_name, scaler in self.scalers.items():
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)
            X_val_scaled = np.nan_to_num(X_val_scaled, nan=0.0, posinf=0.0, neginf=0.0)
            scalers_fitted[scaler_name] = {
                'scaler': scaler,
                'X_train': X_train_scaled,
                'X_val': X_val_scaled,
            }

        # 神经网络配置
        nn_configs = [
            (
                'transformer',
                EnhancedTabularTransformer(
                    input_dim,
                    num_classes=self.num_classes,
                    embed_dim=128,
                    num_heads=8,
                    num_layers=3,
                    ff_dim=256,
                    dropout=0.2,
                ),
                'robust',
            ),
            ('cnn_lstm', CNN_LSTMModel(input_dim, hidden_dim=256, num_layers=2, num_classes=self.num_classes, dropout=0.3, seq_len=16), 'robust'),
        ]

        for model_name, model_class, scaler_name in nn_configs:
            print(f"训练 {model_name}...")
            X_train_scaled = scalers_fitted[scaler_name]['X_train']
            X_val_scaled = scalers_fitted[scaler_name]['X_val']

            # 创建数据加载器 - 只使用训练集，避免数据泄漏
            train_dataset = WellLogDataset(X_train_scaled, y_train.values)
            val_dataset = WellLogDataset(X_val_scaled, y_val.values)

            train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)

            model = model_class.to(device)

            # 训练参数
            criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
            optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
            max_epochs = 30
            scheduler = OneCycleLR(
                optimizer,
                max_lr=0.003,
                epochs=max_epochs,
                steps_per_epoch=len(train_loader),
            )

            epochs = []
            train_losses, val_losses = [], []
            train_accs, val_accs = [], []
            train_f1s, val_f1s = [], []

            best_state = None
            best_f1 = -np.inf
            best_epoch_idx = 0
            best_val_acc = 0.0
            best_train_acc = 0.0
            best_train_f1 = 0.0

            for epoch in range(max_epochs):
                model.train()
                train_loss_sum = 0.0
                train_correct = 0
                train_samples = 0
                train_preds_epoch = []
                train_labels_epoch = []

                for batch_features, batch_labels in train_loader:
                    batch_features = batch_features.to(device)
                    batch_labels = batch_labels.to(device)

                    optimizer.zero_grad()
                    outputs = model(batch_features)
                    loss = criterion(outputs, batch_labels)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    scheduler.step()

                    with torch.no_grad():
                        train_loss_sum += loss.item() * batch_labels.size(0)
                        preds = torch.argmax(outputs, dim=1)
                        train_correct += (preds == batch_labels).sum().item()
                        train_samples += batch_labels.size(0)
                        train_preds_epoch.extend(preds.cpu().numpy())
                        train_labels_epoch.extend(batch_labels.cpu().numpy())

                train_loss_epoch = train_loss_sum / max(train_samples, 1)
                train_acc_epoch = train_correct / max(train_samples, 1)
                train_f1_epoch = f1_score(train_labels_epoch, train_preds_epoch, average='macro', zero_division=0)

                model.eval()
                val_loss_sum = 0.0
                val_correct = 0
                val_samples = 0
                val_preds_epoch = []
                val_labels_epoch = []
                with torch.no_grad():
                    for batch_features, batch_labels in val_loader:
                        batch_features = batch_features.to(device)
                        batch_labels = batch_labels.to(device)
                        outputs = model(batch_features)
                        loss = criterion(outputs, batch_labels)
                        val_loss_sum += loss.item() * batch_labels.size(0)
                        preds = torch.argmax(outputs, dim=1)
                        val_correct += (preds == batch_labels).sum().item()
                        val_samples += batch_labels.size(0)
                        val_preds_epoch.extend(preds.cpu().numpy())
                        val_labels_epoch.extend(batch_labels.cpu().numpy())

                val_loss_epoch = val_loss_sum / max(val_samples, 1)
                val_acc_epoch = val_correct / max(val_samples, 1)
                val_f1_epoch = f1_score(val_labels_epoch, val_preds_epoch, average='macro', zero_division=0)

                epochs.append(epoch)
                train_losses.append(train_loss_epoch)
                val_losses.append(val_loss_epoch)
                train_accs.append(train_acc_epoch)
                val_accs.append(val_acc_epoch)
                train_f1s.append(train_f1_epoch)
                val_f1s.append(val_f1_epoch)

                if val_f1_epoch > best_f1:
                    best_f1 = val_f1_epoch
                    best_epoch_idx = epoch
                    best_state = copy.deepcopy(model.state_dict())
                    best_val_acc = val_acc_epoch
                    best_train_acc = train_acc_epoch
                    best_train_f1 = train_f1_epoch

                if (epoch + 1) % 5 == 0 or epoch == 0:
                    print(
                        f"Epoch {epoch+1}/{max_epochs} -- Train Acc: {train_acc_epoch:.4f}, Val Acc: {val_acc_epoch:.4f}, Val F1: {val_f1_epoch:.4f}"
                    )

            if best_state is not None:
                model.load_state_dict(best_state)

            models[model_name] = {
                'model': model,
                'score': best_f1,
                'scaler': scalers_fitted[scaler_name]['scaler'],
            }
            self.model_performance[model_name] = {
                'train_acc': best_train_acc,
                'train_f1': best_train_f1,
                'val_acc': best_val_acc,
                'val_f1': best_f1,
                'best_epoch': best_epoch_idx,
            }
            self.training_logs[model_name] = {
                'epochs': epochs,
                'train_acc': train_accs,
                'val_acc': val_accs,
                'train_loss': train_losses,
                'val_loss': val_losses,
            }
            self.best_epochs[model_name] = best_epoch_idx
            print(f"{model_name}最佳epoch: {best_epoch_idx}, 验证F1: {best_f1:.4f}, 验证准确率: {best_val_acc:.4f}")

        return models

    def find_best_epoch_fast(self, model, train_loader, val_loader, criterion, optimizer, max_epochs=30):
        """快速寻找最佳epoch"""
        best_val_f1 = 0
        best_val_acc = 0
        best_epoch = 0
        patience = 50
        patience_counter = 0

        for epoch in range(max_epochs):
            # 训练
            model.train()
            for batch_features, batch_labels in train_loader:
                batch_features = batch_features.to(device)
                batch_labels = batch_labels.to(device)

                optimizer.zero_grad()
                outputs = model(batch_features)
                loss = criterion(outputs, batch_labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            # 验证
            model.eval()
            val_preds = []
            val_labels = []
            with torch.no_grad():
                for batch_features, batch_labels in val_loader:
                    batch_features = batch_features.to(device)
                    outputs = model(batch_features)
                    val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())
                    val_labels.extend(batch_labels.cpu().numpy())

            val_f1 = f1_score(val_labels, val_preds, average='macro')
            val_acc = accuracy_score(val_labels, val_preds)

            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                best_val_acc = val_acc
                best_epoch = epoch
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}")

            if patience_counter >= patience:
                break

        print(f"最佳epoch: {best_epoch}, 最佳验证F1: {best_val_f1:.4f}, 最佳验证准确率: {best_val_acc:.4f}")
        return best_epoch, best_val_f1

    # ========= 新增：统一获取各模型概率 =========
    def _predict_proba_all_models(self, models, X):
        """对给定样本 X，返回每个模型的预测概率和对应权重"""
        X_clean = self.robust_clean_data(X.copy())
        all_predictions = []
        weights = []

        # 树模型
        for model_name in ['lgb', 'xgb', 'cat', 'et', 'rf']:
            if model_name in models:
                try:
                    model_info = models[model_name]
                    pred_proba = model_info['model'].predict_proba(X_clean)
                    all_predictions.append(pred_proba)
                    weights.append(model_info['score'])
                except Exception as e:
                    print(f"{model_name} 预测失败: {e}")

        # 神经网络模型
        for model_name in ['transformer', 'cnn_lstm']:
            if model_name in models:
                try:
                    model_info = models[model_name]
                    scaler = model_info['scaler']
                    X_scaled = scaler.transform(X_clean)
                    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)
                    X_tensor = torch.FloatTensor(X_scaled).to(device)
                    model = model_info['model']
                    model.eval()
                    with torch.no_grad():
                        outputs = model(X_tensor)
                        probs = torch.softmax(outputs, dim=1).cpu().numpy()
                    all_predictions.append(probs)
                    weights.append(model_info['score'])
                except Exception as e:
                    print(f"{model_name} 预测失败: {e}")

        # 权重归一（平方放大强者权重）
        weights = np.array([w * w for w in weights], dtype=np.float64)
        weights = weights / (weights.sum() + 1e-12)

        return all_predictions, weights

    def _weighted_ensemble_probs(self, prob_list, weights):
        """将多个概率按权重加权求和得到集成概率（静态权重）"""
        ens = np.zeros_like(prob_list[0], dtype=np.float64)
        for p, w in zip(prob_list, weights):
            ens += p * w
        # 防止数值问题
        ens = np.clip(ens, 1e-9, 1.0)
        ens = ens / ens.sum(axis=1, keepdims=True)
        return ens
    
    def _dynamic_weighted_ensemble_probs(self, prob_list, base_weights):
        """
        动态加权集成：根据每个样本的预测置信度动态调整权重
        
        策略：
        1. 对每个样本，计算各模型的预测置信度（最大概率）
        2. 置信度高的模型获得更大的权重
        3. 结合基础权重（验证F1）和动态置信度权重
        """
        n_samples = prob_list[0].shape[0]
        n_classes = prob_list[0].shape[1]
        n_models = len(prob_list)
        
        # 转换为numpy数组方便计算
        base_weights = np.array(base_weights, dtype=np.float64)
        
        # 初始化集成概率
        ens = np.zeros((n_samples, n_classes), dtype=np.float64)
        
        print(f"使用动态加权集成 ({n_models}个模型, {n_samples}个样本)...")
        
        # 对每个样本动态计算权重
        for i in range(n_samples):
            # 提取所有模型对当前样本的预测概率
            sample_probs = np.array([prob_list[j][i] for j in range(n_models)])  # [n_models, n_classes]
            
            # 计算每个模型的置信度（最大预测概率）
            confidences = np.max(sample_probs, axis=1)  # [n_models]
            # 基于该样本平均置信度的温度：置信度低 -> 温度高（放大差异），置信度高 -> 温度低（平滑权重）
            mean_conf = float(confidences.mean())
            temperature = 1.5 + (1.0 - mean_conf) * 4.0
            temperature = float(np.clip(temperature, 0.5, 5.0))
            
            # 动态权重 = 基础权重(F1) × 置信度权重
            # 置信度权重使用softmax，让高置信度的模型权重更大
            confidence_weights = np.exp(confidences * temperature)  # 每个样本自适应温度
            confidence_weights = confidence_weights / confidence_weights.sum()
            
            # 结合基础权重和置信度权重
            dynamic_weights = base_weights * confidence_weights
            dynamic_weights = dynamic_weights / dynamic_weights.sum()
            
            # 加权求和
            for j in range(n_models):
                ens[i] += sample_probs[j] * dynamic_weights[j]
        
        # 防止数值问题
        ens = np.clip(ens, 1e-9, 1.0)
        ens = ens / ens.sum(axis=1, keepdims=True)
        
        return ens


    def post_process_predictions(self, predictions, probabilities, threshold=0.4, target_ratio=0.16):
        """
        后处理：增加label=0的预测数量

        参数:
        - predictions: 原始预测结果
        - probabilities: 预测概率 (n_samples, 3)
        - threshold: label=0概率的阈值，超过此值的样本更容易被改为0
        - target_ratio: 期望label=0占总样本的比例
        """
        print("\n开始后处理...")
        predictions = predictions.copy()

        # 统计原始预测分布
        original_counts = pd.Series(predictions).value_counts().sort_index()
        print(f"原始预测分布:\n{original_counts}")

        # 找出预测为1或2，但label=0概率较高的样本
        label_0_prob = probabilities[:, 0]  # label=0的概率

        # 找出预测为1或2的样本索引
        candidates = np.where((predictions == 1) | (predictions == 2))[0]

        # 按照label=0的概率排序
        candidates_sorted = candidates[np.argsort(-label_0_prob[candidates])]

        # 计算需要转换多少样本为label=0
        current_label0_count = np.sum(predictions == 0)
        target_label0_count = int(len(predictions) * target_ratio)
        needed_conversions = max(0, target_label0_count - current_label0_count)

        print(f"当前label=0数量: {current_label0_count}")
        print(f"目标label=0数量: {target_label0_count}")
        print(f"需要转换: {needed_conversions}个样本")

        # 从候选样本中选择概率最高且超过阈值的样本
        converted_count = 0
        for idx in candidates_sorted:
            if converted_count >= needed_conversions:
                break

            # 检查label=0的概率是否超过阈值
            if label_0_prob[idx] >= threshold:
                predictions[idx] = 0
                converted_count += 1

        print(f"实际转换了 {converted_count} 个样本")

        # 统计后处理后的预测分布
        final_counts = pd.Series(predictions).value_counts().sort_index()
        print(f"后处理后预测分布:\n{final_counts}")

        return predictions

    def print_confusion_matrix_and_metrics(self, y_true, y_pred, title="分类结果"):
        """打印关键指标：F1和准确率"""
        y_true_array = y_true.values if hasattr(y_true, 'values') else y_true
        y_pred_array = y_pred.values if hasattr(y_pred, 'values') else y_pred
        y_true_array = np.asarray(y_true_array).ravel()
        y_pred_array = np.asarray(y_pred_array).ravel()
        
        f1_macro = f1_score(y_true_array, y_pred_array, average='macro', zero_division=0)
        acc = accuracy_score(y_true_array, y_pred_array)
        kappa = cohen_kappa_score(y_true_array, y_pred_array)
        mcc = matthews_corrcoef(y_true_array, y_pred_array)
        
        print(f"{title}: F1={f1_macro:.4f}, Acc={acc:.4f}, Kappa={kappa:.4f}, MCC={mcc:.4f}")

    def compute_per_class_accuracy(self, y_true, y_pred):
        """计算每个岩性类别的准确率"""
        y_true_array = np.asarray(y_true).ravel()
        y_pred_array = np.asarray(y_pred).ravel()

        per_class_accuracy = {}
        for cls in FACIES_ORDER:
            mask = y_true_array == cls
            if mask.any():
                per_class_accuracy[cls] = accuracy_score(y_true_array[mask], y_pred_array[mask])
            else:
                per_class_accuracy[cls] = np.nan
        return per_class_accuracy
    
    def print_individual_model_results(self, y_true, model_predictions_dict):
        """打印每个模型的验证集结果对比"""
        print("\n各模型验证性能:")
        results = []
        for model_key, y_pred in model_predictions_dict.items():
            y_true_array = np.asarray(y_true).ravel()
            y_pred_array = np.asarray(y_pred).ravel()
            
            f1_macro = f1_score(y_true_array, y_pred_array, average='macro', zero_division=0)
            acc = accuracy_score(y_true_array, y_pred_array)
            kappa = cohen_kappa_score(y_true_array, y_pred_array)
            mcc = matthews_corrcoef(y_true_array, y_pred_array)
            per_class_acc = self.compute_per_class_accuracy(y_true_array, y_pred_array)
            
            # 计算每类的Precision、Recall和F1-score
            precision_per_class = precision_score(y_true_array, y_pred_array, average=None, zero_division=0, labels=FACIES_ORDER)
            recall_per_class = recall_score(y_true_array, y_pred_array, average=None, zero_division=0, labels=FACIES_ORDER)
            f1_per_class = f1_score(y_true_array, y_pred_array, average=None, zero_division=0, labels=FACIES_ORDER)
            
            # 转换为字典格式
            per_class_precision = {FACIES_ORDER[i]: precision_per_class[i] for i in range(len(FACIES_ORDER))}
            per_class_recall = {FACIES_ORDER[i]: recall_per_class[i] for i in range(len(FACIES_ORDER))}
            per_class_f1 = {FACIES_ORDER[i]: f1_per_class[i] for i in range(len(FACIES_ORDER))}
            
            results.append({
                'model_key': model_key,
                'display_name': self.get_display_name(model_key),
                'f1': f1_macro,
                'acc': acc,
                'kappa': kappa,
                'mcc': mcc,
                'per_class_acc': per_class_acc,
                'per_class_precision': per_class_precision,
                'per_class_recall': per_class_recall,
                'per_class_f1': per_class_f1,
            })
        
        results.sort(key=lambda x: x['f1'], reverse=True)
        
        summary = {}
        for r in results:
            per_class_display = []
            for cls_id in FACIES_ORDER:
                acc_val = r['per_class_acc'].get(cls_id, np.nan)
                if np.isnan(acc_val):
                    per_class_display.append(f"{cls_id}:N/A")
                else:
                    per_class_display.append(f"{cls_id}:{acc_val:.4f}")
            per_class_str = " | ".join(per_class_display)
            print(
                f"  {r['display_name']:28s}: F1={r['f1']:.4f}, Acc={r['acc']:.4f}, "
                f"Kappa={r['kappa']:.4f}, MCC={r['mcc']:.4f}, 每类准确率[{per_class_str}]"
            )
            summary[r['model_key']] = {
                'display_name': r['display_name'],
                'f1': r['f1'],
                'acc': r['acc'],
                'kappa': r['kappa'],
                'mcc': r['mcc'],
                'per_class_acc': r['per_class_acc'],
                'per_class_precision': r['per_class_precision'],
                'per_class_recall': r['per_class_recall'],
                'per_class_f1': r['per_class_f1'],
            }
        return summary
    
    def weighted_average_ensemble(self, models, X_val, y_val, X_test):
        """
        1) 验证集：各模型概率 -> 动态加权集成
        2) 测试集：各模型概率 -> 动态加权集成
        """
        print("\n进行动态加权集成...")

        if hasattr(y_val, 'reset_index'):
            y_val_series = y_val.reset_index(drop=True)
        else:
            y_val_series = pd.Series(np.asarray(y_val)).reset_index(drop=True)

        # 验证集集成
        val_prob_list, weights = self._predict_proba_all_models(models, X_val)
        print(f"基础权重（基于验证F1）: {weights}")
        
        # 静态加权集成
        val_probs_static = self._weighted_ensemble_probs(val_prob_list, weights)
        val_pred_static = val_probs_static.argmax(axis=1)
        val_f1_static = f1_score(y_val_series, val_pred_static, average='macro')
        val_acc_static = accuracy_score(y_val_series, val_pred_static)
        print(f"静态加权 验证集 宏F1: {val_f1_static:.4f}, 准确率: {val_acc_static:.4f}")
        
        # 动态加权集成
        val_probs_ens = self._dynamic_weighted_ensemble_probs(val_prob_list, weights)
        val_pred_labels = val_probs_ens.argmax(axis=1)
        val_f1 = f1_score(y_val_series, val_pred_labels, average='macro')
        val_acc = accuracy_score(y_val_series, val_pred_labels)
        print(f"动态加权 验证集 宏F1: {val_f1:.4f}, 准确率: {val_acc:.4f}")

        # 保存静态集成结果
        self.model_performance['ensemble_static'] = {
            'train_acc': np.nan,
            'train_f1': np.nan,
            'val_acc': val_acc_static,
            'val_f1': val_f1_static,
            'best_epoch': None,
        }
        
        # 保存动态集成结果
        self.model_performance['ensemble_dynamic'] = {
            'train_acc': np.nan,
            'train_f1': np.nan,
            'val_acc': val_acc,
            'val_f1': val_f1,
            'best_epoch': None,
        }

        self.print_confusion_matrix_and_metrics(y_val_series, val_pred_labels, "动态加权集成验证集")

        self.last_validation_results = {
            'pre_decode_probs': val_probs_ens,
            'pre_decode_preds': val_pred_labels,
            'post_decode_preds': None,
            'pre_decode_acc': val_acc,
            'post_decode_acc': None,
            'static_probs': val_probs_static,
            'static_preds': val_pred_static,
        }

        # 测试集集成（动态加权）
        test_prob_list, _ = self._predict_proba_all_models(models, X_test)
        test_probs_ens = self._dynamic_weighted_ensemble_probs(test_prob_list, weights)
        final_predictions = test_probs_ens.argmax(axis=1)
        predicted_probabilities = test_probs_ens[np.arange(len(test_probs_ens)), final_predictions]

        self.last_test_probabilities = test_probs_ens
        self.last_test_predicted_probabilities = predicted_probabilities

        return {
            'test_predictions': final_predictions,
            'test_probabilities': test_probs_ens,
            'test_predicted_probabilities': predicted_probabilities,
            'val_pred_static': val_pred_static,
            'val_probs_static': val_probs_static,
            'val_pred_dynamic': val_pred_labels,
            'val_probs_dynamic': val_probs_ens,
        }

    def print_ablation_summary(self):
        """输出各模型在训练/验证集上的消融实验结果"""
        if not self.model_performance:
            print("\n暂无消融实验结果可展示。")
            return
        rows = []
        for model_name, metrics in self.model_performance.items():
            rows.append(
                {
                    'model': self.get_display_name(model_name),
                    'best_epoch': metrics.get('best_epoch'),
                    'train_acc': metrics.get('train_acc'),
                    'train_f1': metrics.get('train_f1'),
                    'val_acc': metrics.get('val_acc'),
                    'val_f1': metrics.get('val_f1'),
                }
            )
        summary_df = pd.DataFrame(rows)
        summary_df = summary_df.sort_values(by='val_f1', ascending=False)
        print("\n================ 消融实验 - 各模型验证表现 ================")
        print(summary_df.to_string(index=False, float_format=lambda x: f"{x:.4f}"))
        output_path = tagged_name('model_ablation_summary.csv')
        summary_df.to_csv(output_path, index=False)
        print(f"结果已保存至 {output_path}")

    def plot_model_boxplots(self, per_class_summary, output_dir='.'):
        """绘制所有模型的每类准确率箱线图"""
        if not per_class_summary:
            print("没有可用的模型性能数据，跳过箱线图绘制")
            return
        
        data = []
        labels = []
        for model_key, metrics in per_class_summary.items():
            per_class_acc = metrics.get('per_class_acc', {})
            values = [acc_val * 100.0 for acc_val in [per_class_acc.get(fid, np.nan) for fid in FACIES_ORDER]
                      if not np.isnan(acc_val)]
            if not values:
                continue
            data.append(values)
            labels.append(metrics.get('display_name', self.get_display_name(model_key)))

        if not data:
            print("所有模型缺少有效的每类准确率，跳过箱线图绘制")
            return

        fig_width = max(9.0, 1.2 * len(labels) + 4)
        fig, ax = plt.subplots(figsize=(fig_width, 6))

        boxprops = dict(color='#1f77b4', linewidth=1.4)
        whiskerprops = dict(color='#1f77b4', linewidth=1.2, linestyle='--')
        capprops = dict(color='#1f77b4', linewidth=1.2)
        medianprops = dict(color='#d62728', linewidth=1.5)
        flierprops = dict(marker='o', markerfacecolor='#1f77b4', markeredgecolor='#1f77b4', markersize=3, alpha=0.6)

        bp = ax.boxplot(
            data,
            labels=labels,
            patch_artist=True,
            widths=0.55,
            boxprops=boxprops,
            whiskerprops=whiskerprops,
            capprops=capprops,
            medianprops=medianprops,
            flierprops=flierprops,
        )

        for box in bp['boxes']:
            box.set_facecolor('#FFFFFF')

        ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
        ax.set_xlabel('Models', fontsize=12, fontweight='bold')
        ax.set_title('Per-Class Accuracy Distribution Across Models', fontsize=14, fontweight='bold')
        ax.set_ylim(0, 100)
        ax.set_yticks(np.arange(0, 101, 10))
        ax.yaxis.grid(True, linestyle='--', alpha=0.3)
        ax.tick_params(axis='x', labelrotation=25, labelsize=10)
        ax.tick_params(axis='y', labelsize=10)

        plt.tight_layout()

        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, tagged_name('model_per_class_accuracy_boxplot.png'))
        fig.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"箱线图已保存: {output_path}")

    def plot_pairwise_comparison(self, per_class_summary, output_dir='.'):
        """绘制模型之间基于每类准确率的成对比较图"""
        if not per_class_summary:
            print("没有可用的模型性能数据，跳过成对比较图绘制")
            return

        model_names = list(per_class_summary.keys())
        n_models = len(model_names)
        if n_models < 2:
            print("模型数量不足，无法生成成对比较图")
            return

        fig, axes = plt.subplots(n_models, n_models, figsize=(3.6 * n_models, 3.6 * n_models))

        for i, model_i in enumerate(model_names):
            for j, model_j in enumerate(model_names):
                ax = axes[i, j]
                display_i = per_class_summary[model_i].get('display_name', self.get_display_name(model_i))
                display_j = per_class_summary[model_j].get('display_name', self.get_display_name(model_j))

                if i == j:
                    metrics = per_class_summary[model_i]
                    ax.axis('off')
                    ax.text(0.5, 0.65, display_i, ha='center', va='center', fontsize=12, fontweight='bold')
                    ax.text(
                        0.5,
                        0.35,
                        f"F1={metrics['f1']*100:.2f}%\nAcc={metrics['acc']*100:.2f}%",
                        ha='center',
                        va='center',
                        fontsize=10
                    )
                    continue

                acc_i = per_class_summary[model_i]['per_class_acc']
                acc_j = per_class_summary[model_j]['per_class_acc']

                if j > i:
                    diffs = []
                    for facies_id in FACIES_ORDER:
                        val_i = acc_i.get(facies_id, np.nan)
                        val_j = acc_j.get(facies_id, np.nan)
                        if not np.isnan(val_i) and not np.isnan(val_j):
                            diffs.append((val_i - val_j) * 100.0)
                    if diffs:
                        mean_diff = float(np.mean(diffs))
                        std_diff = float(np.std(diffs))
                        ax.set_facecolor('#E8F5E9' if mean_diff >= 0 else '#FFEBEE')
                        ax.text(0.5, 0.6, f'Δ={mean_diff:.2f}%', ha='center', va='center', fontsize=11, fontweight='bold')
                        ax.text(0.5, 0.35, f'σ={std_diff:.2f}%', ha='center', va='center', fontsize=10)
                    else:
                        ax.set_facecolor('#ECEFF1')
                        ax.text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=11, fontweight='bold')
                    ax.set_xticks([])
                    ax.set_yticks([])
                else:
                    xs, ys, colors = [], [], []
                    for facies_id in FACIES_ORDER:
                        val_i = acc_i.get(facies_id, np.nan)
                        val_j = acc_j.get(facies_id, np.nan)
                        if not np.isnan(val_i) and not np.isnan(val_j):
                            xs.append(val_j * 100.0)
                            ys.append(val_i * 100.0)
                            colors.append(FACIES_COLOR_MAP.get(facies_id, '#9E9E9E'))
                    if xs:
                        ax.scatter(xs, ys, c=colors, s=80, edgecolors='black', linewidths=0.6, alpha=0.85)
                        ax.plot([0, 100], [0, 100], 'k--', linewidth=1, alpha=0.4)
                        ax.set_xlim(0, 100)
                        ax.set_ylim(0, 100)
                        ax.set_xlabel(f'{display_j} (%)', fontsize=9)
                        ax.set_ylabel(f'{display_i} (%)', fontsize=9)
                        ax.grid(True, alpha=0.3, linestyle='--')
                    else:
                        ax.axis('off')

        legend_handles = [
            Line2D([0], [0], marker='o', color='w', label=FACIES_NAMES.get(facies_id, f'Facies {facies_id}'),
                   markerfacecolor=FACIES_COLOR_MAP.get(facies_id, '#9E9E9E'),
                   markeredgecolor='black', markersize=8)
            for facies_id in FACIES_ORDER
        ]
        fig.legend(legend_handles, [FACIES_NAMES.get(fid, f'Facies {fid}') for fid in FACIES_ORDER], loc='lower center', ncol=len(FACIES_ORDER))
        plt.suptitle('Pairwise Model Comparison (Per-Class Accuracy)', fontsize=15, fontweight='bold')
        plt.tight_layout(rect=[0, 0.04, 1, 0.96])

        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, tagged_name('pairwise_model_comparison.png'))
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"成对比较图已保存: {output_path}")

    def export_per_class_accuracy(self, per_class_summary, output_dir='.'):
        """导出所有模型的每类准确率和整体指标"""
        if not per_class_summary:
            print("没有可用的模型性能数据，跳过准确率导出")
            return

        rows = []
        for model_key, metrics in per_class_summary.items():
            display_name = metrics.get('display_name', self.get_display_name(model_key))
            row = {
                'model_key': model_key,
                'model_name': display_name,
                'macro_f1_%': metrics.get('f1', np.nan) * 100.0 if metrics.get('f1') is not None else np.nan,
                'overall_acc_%': metrics.get('acc', np.nan) * 100.0 if metrics.get('acc') is not None else np.nan,
                'kappa': metrics.get('kappa', np.nan),
                'mcc': metrics.get('mcc', np.nan),
            }
            per_class_acc = metrics.get('per_class_acc', {})
            per_class_precision = metrics.get('per_class_precision', {})
            per_class_recall = metrics.get('per_class_recall', {})
            per_class_f1 = metrics.get('per_class_f1', {})
            
            # 为每个岩性类别添加准确率、Precision、Recall和F1-score
            for facies_id in FACIES_ORDER:
                acc_val = per_class_acc.get(facies_id, np.nan)
                precision_val = per_class_precision.get(facies_id, np.nan)
                recall_val = per_class_recall.get(facies_id, np.nan)
                f1_val = per_class_f1.get(facies_id, np.nan)
                
                row[f'facies_{facies_id}_acc_%'] = acc_val * 100.0 if not np.isnan(acc_val) else np.nan
                row[f'facies_{facies_id}_precision_%'] = precision_val * 100.0 if not np.isnan(precision_val) else np.nan
                row[f'facies_{facies_id}_recall_%'] = recall_val * 100.0 if not np.isnan(recall_val) else np.nan
                row[f'facies_{facies_id}_f1_%'] = f1_val * 100.0 if not np.isnan(f1_val) else np.nan
            rows.append(row)

        summary_df = pd.DataFrame(rows)
        summary_df = summary_df.sort_values(by='macro_f1_%', ascending=False)
        summary_df = summary_df.set_index('model_name')
        summary_df = summary_df.drop(columns=['model_key'])

        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, tagged_name('per_class_accuracy_summary.csv'))
        summary_df.to_csv(output_path, float_format='%.4f')

        print("\n各模型岩相准确率汇总 (百分比):")
        print("=" * 120)
        print(summary_df.to_string(float_format=lambda x: f"{x:.2f}"))
        print("=" * 120)
        print(f"\n准确率汇总已保存: {output_path}")
        
        # 打印详细的每类指标表格
        print("\n各模型每类岩性详细指标汇总 (百分比):")
        print("=" * 120)
        for model_key, metrics in per_class_summary.items():
            display_name = metrics.get('display_name', self.get_display_name(model_key))
            per_class_acc = metrics.get('per_class_acc', {})
            per_class_precision = metrics.get('per_class_precision', {})
            per_class_recall = metrics.get('per_class_recall', {})
            per_class_f1 = metrics.get('per_class_f1', {})
            
            print(f"\n{display_name}:")
            print(f"{'岩性类别':<12} {'准确率(%)':<12} {'Precision(%)':<15} {'Recall(%)':<12} {'F1-score(%)':<12}")
            print("-" * 70)
            for facies_id in FACIES_ORDER:
                acc_val = per_class_acc.get(facies_id, np.nan)
                precision_val = per_class_precision.get(facies_id, np.nan)
                recall_val = per_class_recall.get(facies_id, np.nan)
                f1_val = per_class_f1.get(facies_id, np.nan)
                
                acc_str = f"{acc_val * 100.0:.2f}" if not np.isnan(acc_val) else "N/A"
                precision_str = f"{precision_val * 100.0:.2f}" if not np.isnan(precision_val) else "N/A"
                recall_str = f"{recall_val * 100.0:.2f}" if not np.isnan(recall_val) else "N/A"
                f1_str = f"{f1_val * 100.0:.2f}" if not np.isnan(f1_val) else "N/A"
                
                print(f"Label {facies_id:<8} {acc_str:<12} {precision_str:<15} {recall_str:<12} {f1_str:<12}")
        print("=" * 120)

    def train_and_predict(self, train_path, test_path):
        """主训练预测流程"""
        # 加载数据
        print("加载数据...")
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)

        print(f"训练数据形状: {train_df.shape}")
        print(f"测试数据形状: {test_df.shape}")

        # 特征工程
        print("\n进行超级特征工程...")
        train_fe = self.create_ultra_features(train_df)
        test_fe = self.create_ultra_features(test_df)

        # 根据井或随机划分训练/验证索引
        wells_val = None
        depths_val = None
        if 'WELL' in train_fe.columns:
            unique_wells = list(train_fe['WELL'].unique())
            validation_well = 298
            if validation_well in unique_wells:
                unique_wells = [w for w in unique_wells if w != validation_well]
                val_wells = [validation_well]
            else:
                val_wells = []

            if not unique_wells:
                raise ValueError("训练数据中不包含除验证井以外的其他井，无法训练。")

            train_wells = sorted(unique_wells)
            if len(train_wells) < 2:
                print(f"警告: 训练井数量只有 {len(train_wells)} 口")

            print("\n按井划分数据集 (固定策略)：")
            print(f"训练井 ({len(train_wells)}口): {train_wells}")
            for well in train_wells:
                print(f"  - {well}: {len(train_fe[train_fe['WELL'] == well])} 个样本")

            if val_wells:
                print(f"验证井 ({len(val_wells)}口): {val_wells}")
                for well in val_wells:
                    print(f"  - {well}: {len(train_fe[train_fe['WELL'] == well])} 个样本")
                train_idx = pd.Index(train_fe[train_fe['WELL'].isin(train_wells)].index)
                val_idx = pd.Index(train_fe[train_fe['WELL'].isin(val_wells)].index)
                wells_val = train_fe.loc[val_idx, 'WELL'].reset_index(drop=True)
                depths_val = train_fe.loc[val_idx, 'DEPTH'].reset_index(drop=True)
            else:
                print("未指定验证井，将按75/25随机划分")
                idx_train, idx_val = train_test_split(
                    train_fe.index,
                    test_size=0.25,
                    random_state=42,
                    stratify=train_fe['label'],
                )
                train_idx = pd.Index(idx_train)
                val_idx = pd.Index(idx_val)
                wells_val = train_fe.loc[val_idx, 'WELL'].reset_index(drop=True)
                depths_val = train_fe.loc[val_idx, 'DEPTH'].reset_index(drop=True)
        else:
            idx_train, idx_val = train_test_split(
                train_fe.index,
                test_size=0.2,
                random_state=42,
                stratify=train_fe['label'],
            )
            train_idx = pd.Index(idx_train)
            val_idx = pd.Index(idx_val)

        # 使用训练子集拟合两阶段模型并生成概率特征
        self._attach_two_stage_features(train_fe, test_fe, train_idx)

        # 选择特征并准备数据
        non_feature_cols = ['WELL', 'DEPTH', 'label', 'ID', 'id']
        self.feature_columns = [
            col
            for col in train_fe.columns
            if col not in non_feature_cols
            and col in test_fe.columns
            and train_fe[col].dtype in ['float64', 'int64']
        ]
        print(f"最终特征数量: {len(self.feature_columns)}")

        X_train = self.robust_clean_data(train_fe[self.feature_columns])
        y_train = train_fe['label']
        X_test = self.robust_clean_data(test_fe[self.feature_columns]).reset_index(drop=True)

        X_train_split = X_train.loc[train_idx].reset_index(drop=True)
        y_train_split = y_train.loc[train_idx].reset_index(drop=True)
        X_val_split = X_train.loc[val_idx].reset_index(drop=True)
        y_val_split = y_train.loc[val_idx].reset_index(drop=True)

        if wells_val is None and 'WELL' in train_fe.columns:
            wells_val = train_fe.loc[val_idx, 'WELL'].reset_index(drop=True)
            depths_val = train_fe.loc[val_idx, 'DEPTH'].reset_index(drop=True)

        print(f"训练集: {X_train_split.shape}, 验证集: {X_val_split.shape}")

        # 分析类别0和1的区分性特征
        print("\n" + "="*80)
        print("分析类别0（粉砂岩）和类别1（砂岩）的区分性特征...")
        print("="*80)
        y_train_array = np.asarray(y_train_split)
        mask_0_or_1 = (y_train_array == 0) | (y_train_array == 1)
        
        if mask_0_or_1.sum() > 0:
            X_01 = X_train_split[mask_0_or_1]
            y_01 = y_train_array[mask_0_or_1]
            
            # 使用互信息来评估特征对区分类别0和1的重要性
            try:
                mi_scores = mutual_info_classif(X_01, y_01, random_state=42)
                mi_features = pd.DataFrame({
                    'feature': X_train_split.columns,
                    'mutual_info_score': mi_scores
                }).sort_values('mutual_info_score', ascending=False)
                
                print(f"\n对区分类别0和1最重要的前20个特征（互信息）：")
                for idx, row in mi_features.head(20).iterrows():
                    print(f"  {row['feature']:50s}: {row['mutual_info_score']:.6f}")
                
                # 使用F统计量评估特征重要性
                f_scores, p_values = f_classif(X_01, y_01)
                f_features = pd.DataFrame({
                    'feature': X_train_split.columns,
                    'f_score': f_scores,
                    'p_value': p_values
                }).sort_values('f_score', ascending=False)
                
                print(f"\n对区分类别0和1最重要的前20个特征（F统计量）：")
                for idx, row in f_features.head(20).iterrows():
                    print(f"  {row['feature']:50s}: F={row['f_score']:.2f}, p={row['p_value']:.6f}")
                
                # 保存最重要的区分性特征
                top_features = set(
                    list(mi_features.head(30)['feature']) + 
                    list(f_features.head(30)['feature'])
                )
                print(f"\n识别出 {len(top_features)} 个重要的区分性特征")
                
            except Exception as e:
                print(f"特征分析时出错: {e}")
        
        # 特征选择：删除没用的特征
        print("\n" + "="*80)
        print("执行特征选择，删除不重要和冗余的特征...")
        print("="*80)
        
        try:
            # 方法1: 使用互信息评估所有类别的特征重要性
            print("\n使用互信息评估所有类别的特征重要性...")
            mi_scores_all = mutual_info_classif(X_train_split, y_train_split, random_state=42)
            mi_features_all = pd.DataFrame({
                'feature': X_train_split.columns,
                'mi_score': mi_scores_all
            }).sort_values('mi_score', ascending=False)
            
            # 方法2: 使用F统计量评估特征重要性
            print("使用F统计量评估特征重要性...")
            f_scores_all, p_values_all = f_classif(X_train_split, y_train_split)
            f_features_all = pd.DataFrame({
                'feature': X_train_split.columns,
                'f_score': f_scores_all,
                'p_value': p_values_all
            }).sort_values('f_score', ascending=False)
            
            # 方法3: 使用LightGBM快速训练获取特征重要性
            print("使用LightGBM评估特征重要性...")
            try:
                lgb_quick = lgb.LGBMClassifier(
                    n_estimators=50,
                    random_state=42,
                    verbose=-1,
                    n_jobs=-1
                )
                lgb_quick.fit(X_train_split, y_train_split)
                lgb_importances = pd.DataFrame({
                    'feature': X_train_split.columns,
                    'lgb_importance': lgb_quick.feature_importances_
                }).sort_values('lgb_importance', ascending=False)
            except Exception as e:
                print(f"LightGBM特征重要性评估失败: {e}")
                lgb_importances = pd.DataFrame({
                    'feature': X_train_split.columns,
                    'lgb_importance': np.zeros(len(X_train_split.columns))
                })
            
            # 综合三种方法选择特征
            # 归一化三种评分到0-1范围
            mi_features_all['mi_score_norm'] = (mi_features_all['mi_score'] - mi_features_all['mi_score'].min()) / (mi_features_all['mi_score'].max() - mi_features_all['mi_score'].min() + 1e-8)
            f_features_all['f_score_norm'] = (f_features_all['f_score'] - f_features_all['f_score'].min()) / (f_features_all['f_score'].max() - f_features_all['f_score'].min() + 1e-8)
            lgb_importances['lgb_importance_norm'] = (lgb_importances['lgb_importance'] - lgb_importances['lgb_importance'].min()) / (lgb_importances['lgb_importance'].max() - lgb_importances['lgb_importance'].min() + 1e-8)
            
            # 合并三种评分
            feature_importance = mi_features_all[['feature', 'mi_score_norm']].merge(
                f_features_all[['feature', 'f_score_norm']], on='feature', how='outer'
            ).merge(
                lgb_importances[['feature', 'lgb_importance_norm']], on='feature', how='outer'
            )
            feature_importance['combined_score'] = (
                feature_importance['mi_score_norm'] * 0.4 +
                feature_importance['f_score_norm'] * 0.3 +
                feature_importance['lgb_importance_norm'] * 0.3
            )
            feature_importance = feature_importance.sort_values('combined_score', ascending=False)
            
            # 选择Top K特征（保留前80%的特征，或至少500个特征）
            num_features_before = len(X_train_split.columns)
            k_features = max(500, int(num_features_before * 0.8))
            k_features = min(k_features, num_features_before)
            
            selected_features = feature_importance.head(k_features)['feature'].tolist()
            
            # 保存筛选后的特征及评分
            selected_scores_path = "selected_features_scores.csv"
            selected_scores = feature_importance[
                feature_importance['feature'].isin(selected_features)
            ][['feature', 'combined_score', 'mi_score_norm', 'f_score_norm', 'lgb_importance_norm']]
            selected_scores.to_csv(selected_scores_path, index=False, float_format="%.6f")
            print(f"已保存筛选特征评分: {selected_scores_path}")
            
            print(f"\n特征选择结果:")
            print(f"  原始特征数量: {num_features_before}")
            print(f"  筛选后特征数量: {len(selected_features)}")
            print(f"  删除特征数量: {num_features_before - len(selected_features)}")
            print(f"  特征减少率: {(1 - len(selected_features) / num_features_before) * 100:.2f}%")
            
            print(f"\n最重要的前20个特征（综合评分）:")
            for idx, row in feature_importance.head(20).iterrows():
                print(f"  {row['feature']:50s}: 综合评分={row['combined_score']:.4f} (MI={row['mi_score_norm']:.3f}, F={row['f_score_norm']:.3f}, LGB={row['lgb_importance_norm']:.3f})")
            
            # 应用特征选择：只保留选中的特征，删除没用的特征
            X_train_split = X_train_split[selected_features]
            X_val_split = X_val_split[selected_features]
            self.feature_columns = selected_features
            
            # 更新测试集特征
            available_test_features = [f for f in selected_features if f in X_test.columns]
            if len(available_test_features) < len(selected_features):
                print(f"警告: 测试集中缺少 {len(selected_features) - len(available_test_features)} 个特征")
                # 为缺失的特征填充0
                missing_features = [f for f in selected_features if f not in X_test.columns]
                for feat in missing_features:
                    X_test[feat] = 0.0
            X_test = X_test[selected_features]
            
            print(f"\n特征选择完成，已删除 {num_features_before - len(selected_features)} 个无用特征")
            print(f"训练集形状: {X_train_split.shape}, 验证集形状: {X_val_split.shape}, 测试集形状: {X_test.shape}")
            
        except Exception as e:
            print(f"特征选择时出错: {e}")
            print("使用所有特征继续训练...")
            import traceback
            traceback.print_exc()
        
        print("="*80 + "\n")

        # 训练所有模型
        all_models = {}
        individual_val_predictions = {}  # 保存每个模型的验证集预测

        # 训练树模型
        tree_models = self.train_tree_models(X_train_split, y_train_split, X_val_split, y_val_split)
        all_models.update(tree_models)

        # 训练神经网络模型
        neural_models = self.train_neural_models(
            X_train_split, y_train_split, X_val_split, y_val_split
        )
        all_models.update(neural_models)
        
        # 收集每个模型的验证集预测
        print("\n" + "="*80)
        print("收集各个模型的验证集预测...")
        print("="*80)
        for model_name, model_info in all_models.items():
            try:
                X_val_clean = self.robust_clean_data(X_val_split.copy())
                
                if model_name in ['transformer', 'cnn_lstm']:
                    # 神经网络模型
                    scaler = model_info['scaler']
                    X_val_scaled = scaler.transform(X_val_clean)
                    X_val_scaled = np.nan_to_num(X_val_scaled, nan=0.0, posinf=0.0, neginf=0.0)
                    X_tensor = torch.FloatTensor(X_val_scaled).to(device)
                    model = model_info['model']
                    model.eval()
                    with torch.no_grad():
                        outputs = model(X_tensor)
                        val_pred = torch.argmax(outputs, dim=1).cpu().numpy()
                else:
                    # 树模型
                    val_pred = model_info['model'].predict(X_val_clean)
                
                individual_val_predictions[model_name] = val_pred
            except Exception as e:
                print(f"收集 {model_name} 预测时出错: {e}")
        
        # 集成 + 概率校准 + 维特比
        ensemble_outputs = self.weighted_average_ensemble(
            all_models, X_val_split, y_val_split, X_test
        )

        test_predictions = ensemble_outputs['test_predictions']
        test_probabilities = ensemble_outputs['test_probabilities']
        test_predicted_probabilities = ensemble_outputs['test_predicted_probabilities']
        val_pred_static = ensemble_outputs['val_pred_static']
        val_pred_dynamic = ensemble_outputs['val_pred_dynamic']

        all_val_predictions = individual_val_predictions.copy()
        all_val_predictions['ensemble_static'] = val_pred_static
        all_val_predictions['ensemble_dynamic'] = val_pred_dynamic

        # 打印每个模型的详细结果（包含集成模型）
        per_class_summary = self.print_individual_model_results(y_val_split, all_val_predictions)
        self.print_ablation_summary()

        evaluation_dir = 'model_evaluation_outputs'
        os.makedirs(evaluation_dir, exist_ok=True)
        print(f"\n模型评估图表将保存到: {evaluation_dir}")

        # 保存每个模型的验证集预测结果
        per_model_dir = os.path.join(evaluation_dir, 'per_model_predictions')
        os.makedirs(per_model_dir, exist_ok=True)

        base_val_df = pd.DataFrame({
            'val_index': val_idx.to_numpy() if isinstance(val_idx, pd.Index) else np.arange(len(y_val_split)),
            'true_label': np.asarray(y_val_split).ravel(),
        })
        if wells_val is not None:
            base_val_df['WELL'] = np.asarray(wells_val)
        if depths_val is not None:
            base_val_df['DEPTH'] = np.asarray(depths_val)

        for model_key, preds in all_val_predictions.items():
            model_df = base_val_df.copy()
            model_df['pred_label'] = np.asarray(preds).ravel()
            file_name = f"{model_key}_val_predictions.csv"
            model_df.to_csv(os.path.join(per_model_dir, file_name), index=False)
            print(f"保存模型 {self.get_display_name(model_key)} 的验证预测: {file_name}")

        self.plot_model_boxplots(per_class_summary, output_dir=evaluation_dir)
        self.plot_pairwise_comparison(per_class_summary, output_dir=evaluation_dir)
        self.export_per_class_accuracy(per_class_summary, output_dir=evaluation_dir)

        # 创建提交文件（直接使用原始预测，不进行后处理）
        id_column = 'ID' if 'ID' in test_df.columns else 'id'
        submission_ids = (
            test_df[id_column].values if id_column in test_df.columns else range(len(test_predictions))
        )
        submission = pd.DataFrame({
            'id': submission_ids,
            'predict': test_predictions,
            'probability': test_predicted_probabilities,
        })
        return submission


# 主程序
if __name__ == "__main__":
    identifier = UltraFastLithologyIdentifier()
    try:
        submission = identifier.train_and_predict(
            "train.csv",
            "validation_without_label.csv",
        )
        if submission is not None:
            submission_path = tagged_name("submission_fixed_leak.csv")
            submission.to_csv(submission_path, index=False)
            print(f"\n预测完成！结果已保存到 {submission_path}")
            print(f"提交文件形状: {submission.shape}")
            print(f"预测值分布:\n{submission['predict'].value_counts().sort_index()}")
        else:
            print("训练失败，无法生成预测结果")
    except Exception as e:
        print(f"程序运行出错: {e}")
        import traceback

        traceback.print_exc()
